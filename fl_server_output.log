[93mWARNING [0m:   DEPRECATED FEATURE: flwr.server.start_server() is deprecated.
	Instead, use the `flower-superlink` CLI command to start a SuperLink as shown below:

		$ flower-superlink --insecure

	To view usage and all available options, run:

		$ flower-superlink --help

	Using `start_server()` is deprecated.

            This is a deprecated feature. It will be removed
            entirely in future versions of Flower.
        
[92mINFO [0m:      Starting Flower server, config: num_rounds=5, no round_timeout
[92mINFO [0m:      Flower ECE: gRPC server running (5 rounds), SSL is disabled
[92mINFO [0m:      [INIT]
[92mINFO [0m:      Requesting initial parameters from one random client
[92mINFO [0m:      Received initial parameters from one random client
[92mINFO [0m:      Starting evaluation of initial global parameters
[92mINFO [0m:      Evaluation returned no results (`None`)
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 1]
[92mINFO [0m:      configure_fit: strategy sampled 2 clients (out of 2)
[92mINFO [0m:      aggregate_fit: received 2 results and 0 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 3 clients (out of 3)
[92mINFO [0m:      aggregate_evaluate: received 2 results and 1 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 2]
[92mINFO [0m:      configure_fit: strategy sampled 2 clients (out of 2)
[92mINFO [0m:      aggregate_fit: received 2 results and 0 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 2 clients (out of 2)
[92mINFO [0m:      aggregate_evaluate: received 2 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 3]
[92mINFO [0m:      configure_fit: strategy sampled 2 clients (out of 2)
[92mINFO [0m:      aggregate_fit: received 2 results and 0 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 2 clients (out of 2)
[92mINFO [0m:      aggregate_evaluate: received 2 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 4]
[92mINFO [0m:      configure_fit: strategy sampled 2 clients (out of 2)
[92mINFO [0m:      aggregate_fit: received 2 results and 0 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 2 clients (out of 2)
[92mINFO [0m:      aggregate_evaluate: received 2 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 5]
[92mINFO [0m:      configure_fit: strategy sampled 2 clients (out of 2)
[92mINFO [0m:      aggregate_fit: received 2 results and 0 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 3 clients (out of 3)
[92mINFO [0m:      aggregate_evaluate: received 3 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [SUMMARY]
[92mINFO [0m:      Run finished 5 round(s) in 14.44s
[92mINFO [0m:      	History (loss, distributed):
[92mINFO [0m:      		round 1: 0.7063098748524984
[92mINFO [0m:      		round 2: 0.7312911748886108
[92mINFO [0m:      		round 3: 0.7516452272733053
[92mINFO [0m:      		round 4: 0.7763567765553793
[92mINFO [0m:      		round 5: 0.8271670937538147
[92mINFO [0m:      	History (metrics, distributed, fit):
[92mINFO [0m:      	{'num_clients': [(1, 2), (2, 2), (3, 2), (4, 2), (5, 2)],
[92mINFO [0m:      	 'total_samples': [(1, 1500), (2, 1500), (3, 1500), (4, 1500), (5, 1500)]}
[92mINFO [0m:      	History (metrics, distributed, evaluate):
[92mINFO [0m:      	{'auroc': [(1, np.float64(0.47021910107605003)),
[92mINFO [0m:      	           (2, np.float64(0.4639521567749013)),
[92mINFO [0m:      	           (3, np.float64(0.46865975976234964)),
[92mINFO [0m:      	           (4, np.float64(0.46884991039279583)),
[92mINFO [0m:      	           (5, np.float64(0.47231674132461027))],
[92mINFO [0m:      	 'num_clients': [(1, 2), (2, 2), (3, 2), (4, 2), (5, 3)]}
[92mINFO [0m:      
============================================================
Enhanced Federated Learning Server
FedProx + Fairness Weighting + Differential Privacy
============================================================

Configuration:
  Server address: 0.0.0.0:8080
  Num rounds: 5
  FedProx mu: 0.01
  DP epsilon: 5.0, delta: 1e-05
  Fairness weights: {'auroc': 0.6, 'samples': 0.3, 'domain_relevance': 0.1}

Loading domain relevance matrix...
+ Loaded relevance scores for 5 hospitals
+ Blockchain audit log initialized: fl_results\blockchain_audit
+ FedProxFairness strategy initialized
  - Proximal µ: 0.01
  - Fairness weights: {'auroc': 0.6, 'samples': 0.3, 'domain_relevance': 0.1}
  - Log directory: fl_results
  - Blockchain audit: Enabled

============================================================
Starting FL Server...
Waiting for clients to connect on 0.0.0.0:8080
============================================================


============================================================
Round 1: Aggregating 2 clients
============================================================
  A: AUROC=0.488, samples=1,000, weight=0.3937
  B: AUROC=0.472, samples=500, weight=0.2540

Normalized weights:
  A: 0.6079 (60.8%)
  B: 0.3921 (39.2%)

+ Aggregation complete. Log saved to: fl_results\round_1_aggregation.json
+ Blockchain audit updated (Block #4)

Round 1 Evaluation:
  Average Loss: 0.7063
  Average AUROC: 0.4702

============================================================
Round 2: Aggregating 2 clients
============================================================
  B: AUROC=0.472, samples=500, weight=0.2543
  A: AUROC=0.480, samples=1,000, weight=0.3890

Normalized weights:
  B: 0.3952 (39.5%)
  A: 0.6048 (60.5%)

+ Aggregation complete. Log saved to: fl_results\round_2_aggregation.json
+ Blockchain audit updated (Block #5)

Round 2 Evaluation:
  Average Loss: 0.7313
  Average AUROC: 0.4640

============================================================
Round 3: Aggregating 2 clients
============================================================
  B: AUROC=0.460, samples=500, weight=0.2473
  A: AUROC=0.478, samples=1,000, weight=0.3880

Normalized weights:
  B: 0.3893 (38.9%)
  A: 0.6107 (61.1%)

+ Aggregation complete. Log saved to: fl_results\round_3_aggregation.json
+ Blockchain audit updated (Block #6)

Round 3 Evaluation:
  Average Loss: 0.7516
  Average AUROC: 0.4687

============================================================
Round 4: Aggregating 2 clients
============================================================
  B: AUROC=0.468, samples=500, weight=0.2519
  A: AUROC=0.486, samples=1,000, weight=0.3926

Normalized weights:
  B: 0.3908 (39.1%)
  A: 0.6092 (60.9%)

+ Aggregation complete. Log saved to: fl_results\round_4_aggregation.json
+ Blockchain audit updated (Block #7)

Round 4 Evaluation:
  Average Loss: 0.7764
  Average AUROC: 0.4688

============================================================
Round 5: Aggregating 2 clients
============================================================
  A: AUROC=0.492, samples=1,000, weight=0.3964
  B: AUROC=0.477, samples=500, weight=0.2572

Normalized weights:
  A: 0.6065 (60.6%)
  B: 0.3935 (39.4%)

+ Aggregation complete. Log saved to: fl_results\round_5_aggregation.json
+ Blockchain audit updated (Block #8)

Round 5 Evaluation:
  Average Loss: 0.8272
  Average AUROC: 0.4723

============================================================
FL Training Complete!
============================================================
Results saved to: fl_results
